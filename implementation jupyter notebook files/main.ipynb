{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ce35fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tharun/Desktop/final-year proj/datasets/X_val.csv\n",
      "Upload button clicked\n",
      "/Users/tharun/Desktop/final-year proj/datasets/X_val.csv\n",
      "4688/4688 [==============================] - 5s 1ms/step\n",
      "2010/2010 [==============================] - 3s 2ms/step\n",
      "2010/2010 [==============================] - 36s 17ms/step\n",
      "Initial ensemble accuracy: 70.23%\n",
      "Counter({0: 50994, 2: 5803, 4: 4976, 1: 860, 10: 590, 5: 580, 3: 388, 6: 98, 11: 3})\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from imutils import paths\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.losses import MeanSquaredError\n",
    "from keras import datasets, layers, models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import keras\n",
    "from sklearn.utils import resample\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#     import seaborn as sns\n",
    "#     sns.set()\n",
    "\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "#file_path\n",
    "def upload():\n",
    "    global file_path\n",
    "    file_path = filedialog.askopenfilename(filetypes=((\"CSV files\", \"*.csv\"), (\"All files\", \"*.*\")))\n",
    "    output_text.insert(\"end\", file_path + \"\\n\")\n",
    "    output_text.insert(\"end\", \"upload button clicked\\n\")\n",
    "    print(file_path)\n",
    "    #predict(file_path)\n",
    "    print(\"Upload button clicked\")\n",
    "    \n",
    "def run(dataset):\n",
    "    \n",
    "    dff = pd.read_csv(dataset)\n",
    "    dff = dff.fillna(0)\n",
    "    dff = dff.replace([np.inf, -np.inf], 1e9)\n",
    "\n",
    "    col_to_encode = ' Label'\n",
    "    encoder = LabelEncoder()\n",
    "    dff[col_to_encode] = encoder.fit_transform(dff[col_to_encode])\n",
    "\n",
    "  # Separate the features and target\n",
    "    features = dff.iloc[:, :-1]\n",
    "    target = dff.iloc[:, -1]\n",
    "\n",
    "  # Create a MinMaxScaler object\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "  # Normalize the data in each column\n",
    "    features = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)\n",
    "\n",
    "  # split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "\n",
    "    X_train = X_train.values\n",
    "    X_test = X_test.values\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "\n",
    "\n",
    "    y_train = np.squeeze(y_train)\n",
    "    y_test = np.squeeze(y_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define the hyperparameters\n",
    "    input_dim = 78\n",
    "    hidden_dim_1 = 64\n",
    "    hidden_dim_2 = 32\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 32\n",
    "    num_epochs = 1\n",
    "    beta = 1.0  # the coefficient for the contractive penalty term\n",
    "\n",
    "    # Define the layers of the autoencoder\n",
    "    input_layer = tf.keras.layers.Input(shape=(input_dim,))\n",
    "    encoder_1 = tf.keras.layers.Dense(hidden_dim_1, activation=\"relu\")(input_layer)\n",
    "    encoder_2 = tf.keras.layers.Dense(hidden_dim_2, activation=\"relu\", name='encoder_2')(encoder_1)\n",
    "    decoder_1 = tf.keras.layers.Dense(hidden_dim_1, activation=\"relu\")(encoder_2)\n",
    "    decoder_2 = tf.keras.layers.Dense(input_dim, activation=\"sigmoid\")(decoder_1)\n",
    "\n",
    "    # Define the model and compile it\n",
    "    autoencoder = tf.keras.models.Model(inputs=input_layer, outputs=decoder_2)\n",
    "\n",
    "    def contractive_loss(y_true, y_pred):\n",
    "        \"\"\"Calculates the contractive loss for a given batch of input data.\"\"\"\n",
    "        mse = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "        W = K.variable(value=autoencoder.get_layer('dense').get_weights()[0]) # Get the weight matrix of the first hidden layer\n",
    "        # Compute the jacobian matrix of the hidden layer outputs with respect to the input layer inputs\n",
    "        h = autoencoder.get_layer('encoder_2').output\n",
    "        dh = h * (1 - h) # Derivative of the sigmoid activation function\n",
    "        jacobian = dh[:, None] * W.T[None, :, :] # Compute the jacobian matrix\n",
    "        jacobian = tf.reduce_sum(tf.square(jacobian), axis=(1, 2))\n",
    "        return mse + 1e-4 * jacobian\n",
    "    \n",
    "    \n",
    "    encoder_1 = tf.keras.models.Model(inputs=input_layer, outputs=encoder_1)\n",
    "    encoder_2 = tf.keras.models.Model(inputs=encoder_1.input, outputs=encoder_2)\n",
    "    encoder_2.load_weights('/Users/tharun/Desktop/final-year proj/Models/encoder_2_weights.h5')\n",
    "    \n",
    "    encoded_train = encoder_2.predict(X_train)\n",
    "    encoded_test = encoder_2.predict(X_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    features = [1, 2, 3, 5, 6, 7, 8, 10, 11, 12, 13, 14, 16, 17, 19, 22, 23, 24, 26, 27, 28, 29, 30, 31]\n",
    "\n",
    "  # Convert the NumPy array to a pandas DataFrame\n",
    "    fea_train = pd.DataFrame(encoded_train)\n",
    "    fea_test = pd.DataFrame(encoded_test)\n",
    "\n",
    "\n",
    "    fea_train1= fea_train[features]\n",
    "    fea_test1=fea_test[features]\n",
    "\n",
    "\n",
    "    fea_train1 = np.array(fea_train1)\n",
    "    fea_test1 = np.array(fea_test1)\n",
    "\n",
    "    from keras.models import load_model\n",
    "    model3=load_model('/Users/tharun/Desktop/final-year proj/Models/LSTM_final.h5')\n",
    "    import joblib\n",
    "    dt = joblib.load('/Users/tharun/Desktop/final-year proj/Models/dt_model33.joblib')\n",
    "\n",
    "  # Compute initial ensemble accuracy\n",
    "    dt_pred1 = dt.predict(fea_test1)\n",
    "\n",
    "  # accuracy1 = accuracy_score(y_test, dt_pred1)\n",
    "  # print(accuracy1)\n",
    "    lstm_pred_prob = model3.predict(fea_test1)\n",
    "    lstm_pred1 = np.argmax(lstm_pred_prob, axis=1)\n",
    "\n",
    "    weights=[0.98487615, 0.01706361]\n",
    "\n",
    "    ensemble_pred1 = np.average([dt_pred1, lstm_pred1], axis=0, weights=weights)\n",
    "    initial_score = np.mean(ensemble_pred1 == y_test)\n",
    "    output_text.insert(\"end\", \"Initial ensemble accuracy: {:.2f}%\".format(initial_score*100) + \"\\n\")\n",
    "    print(\"Initial ensemble accuracy: {:.2f}%\".format(initial_score*100))\n",
    "    y_pred_ensemble = np.array([np.argmax(np.bincount([dt_pred1[i], lstm_pred1[i]])) for i in range(len(dt_pred1))])\n",
    "    from collections import Counter\n",
    "    count = Counter(y_pred_ensemble)\n",
    "    output_text.insert(\"end\", count)\n",
    "    print(count)\n",
    "\n",
    "    \n",
    "def predict():\n",
    "    output_text.insert(\"end\", file_path + \"\\n\")\n",
    "    print(file_path)\n",
    "    run(file_path)\n",
    "    output_text.insert(\"end\", \"\\n predict button clicked\\n\")\n",
    "    \n",
    "    \n",
    "root = tk.Tk()\n",
    "root.title(\"My App\")\n",
    "\n",
    "predict_button = tk.Button(root, text=\"Predict\", command=predict)\n",
    "predict_button.pack(side=\"left\", padx=10, pady=10)\n",
    "\n",
    "upload_button = tk.Button(root, text=\"Upload\", command=upload)\n",
    "upload_button.pack(side=\"right\", padx=10, pady=10)\n",
    "\n",
    "\n",
    "output_text = tk.Text(root)\n",
    "output_text.pack(side=\"bottom\", fill=\"both\", expand=True)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb8876d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
